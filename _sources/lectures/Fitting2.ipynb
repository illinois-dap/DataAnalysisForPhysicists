{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting, part 2"
      ],
      "metadata": {
        "id": "8ATy3R6D1-zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'll look at some special situations in fitting"
      ],
      "metadata": {
        "id": "NqCagTMN2Iex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numba_stats\n",
        "%pip install iminuit\n",
        "%pip install jacobi\n",
        "import iminuit\n",
        "from iminuit import cost, Minuit\n",
        "import numba_stats\n",
        "from numba_stats import truncnorm, truncexpon, norm, expon\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import multivariate_normal as mvnorm\n",
        "from jacobi import jacobi\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from iminuit.cost import LeastSquares\n",
        "\n",
        "import seaborn as sns; sns.set()"
      ],
      "metadata": {
        "id": "bvoKs1hE2vFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting small datasets\n",
        "\n",
        "Let's consider a very small dataset....Whereas before we had 1000 samples, now let's only generate 100 (50 from the Gaussian and 50 from the exponential).  Everything else is the same."
      ],
      "metadata": {
        "id": "85FIQE8m2Q1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xr = (0, 2)  # xrange\n",
        "\n",
        "rng = np.random.default_rng(1)\n",
        "\n",
        "xdata = rng.normal(1, 0.1, size=50)\n",
        "ydata = rng.exponential(size=len(xdata))\n",
        "xmix = np.append(xdata, ydata)\n",
        "xmix = xmix[(xr[0] < xmix) & (xmix < xr[1])]\n",
        "\n",
        "#np.histogram returns the values (n in this case) and the bin edges (xe in this case)\n",
        "n, xe = np.histogram(xmix, bins=20, range=xr)\n",
        "cx = 0.5 * (xe[1:] + xe[:-1])\n",
        "dx = np.diff(xe)\n",
        "\n",
        "#this plots \"cx\" which is the middle of every bin \"xe\" and n, the total counts in each bin along with the sqrt(n) as the error bar\n",
        "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3HDZNQJg3d_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf(x, z, mu, sigma, tau):\n",
        "    return (z * truncnorm.pdf(x, *xr, mu, sigma) +\n",
        "            (1 - z) * truncexpon.pdf(x, *xr, 0.0, tau))\n",
        "\n",
        "c = cost.UnbinnedNLL(xmix, pdf)\n",
        "\n",
        "m = Minuit(c, z=0.4, mu=1, sigma=0.2, tau=1)\n",
        "m.limits[\"z\"] = (0, 1)\n",
        "m.limits[\"mu\"] = (0, 2)\n",
        "m.limits[\"sigma\", \"tau\"] = (0, None)\n",
        "m.migrad()"
      ],
      "metadata": {
        "id": "kCF6xTMv2nIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've set the error bars on the histogram just to be the square root of the number of counts in each bin.  That made sense when we had a lot of counts but here it's not the right choice:\n",
        "- the error bars here for empty bins are shown as zero (since 0 is the sqrt of 0) and\n",
        "- 1 for bins with a single count, but we know (from lecture X) that the probablity distribution is just a  Poisson and so you should have *asymmetric* uncertainties here.\n",
        "We could make the bins wider so that we have a larger number of counts/bin and that would make the $\\sqrt{N}$\n",
        "uncertainties correct, but we then lose information about where the points were.\n",
        "\n",
        "However, this unbinned fit doesn't care at all about the number of counts per bin because it doesn't know about them; it just has the data itself, no binning."
      ],
      "metadata": {
        "id": "fP_75r0z2Xpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cdf(xe, z, mu, sigma, tau):\n",
        "    return (z * truncnorm.cdf(xe, *xr, mu, sigma) +\n",
        "            (1-z) * truncexpon.cdf(xe, *xr, 0, tau))\n",
        "\n",
        "c = cost.BinnedNLL(n, xe, cdf)\n",
        "m = Minuit(c, z=0.4, mu=0, sigma=0.2, tau=2)\n",
        "m.limits[\"z\"] = (0, 1)\n",
        "m.limits[\"sigma\", \"tau\"] = (0.01, None)\n",
        "m.migrad()"
      ],
      "metadata": {
        "id": "9dxoq9io2orp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the *binned* fit, but it doesn't take in the errors, it just takes in the *counts* (*n*) and the bins (*xe*) so it doesn't make an assumption about Gaussian uncertainties.  Here it does very similar results between the binned and unbinned fits."
      ],
      "metadata": {
        "id": "9W7LJLhx2d4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make another choice about the binning now; let's go from 20 bins in the histogram to 10.  Where the peak was about 2 bins wide before, now it should be about 1 bin wide.  Everything else, including the underlying data, stays the same."
      ],
      "metadata": {
        "id": "7tyxdmt-XaD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_coarse, xe_coarse = np.histogram(xmix, bins=10, range=xr)\n",
        "c = cost.BinnedNLL(n_coarse, xe_coarse, cdf)\n",
        "m = Minuit(c, z=0.4, mu=0, sigma=0.2, tau=2)\n",
        "m.limits[\"z\"] = (0, 1)\n",
        "m.limits[\"sigma\", \"tau\"] = (0.01, None)\n",
        "m.migrad()"
      ],
      "metadata": {
        "id": "s_sxOMU2XnH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the fit parameters, we see that the $\\sigma$ value is what has changed the most (by a factor of 2.5) and the $\\tau$ parameter has also changed a lot.  $\\mu$ (the mean of the Gaussian) is pretty stable (unsurprisingly).\n",
        "\n",
        "The lesson here is that you want as many bins as your data can support.  In the limit of small statistics, an unbinned fit is best because *all* the available information about the dataset is contained in the fit."
      ],
      "metadata": {
        "id": "517QSHEFYHlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Template Fitting\n",
        "\n",
        "In many situations, one wants to decompose a measured distribution to get a measurement for the fractional contributions of the various distributions.  These fractional contributions often do not have the shape of some fundamental probability distribution (like a Gaussian or an exponential), but are rather measured or come from some Monte Carlo simulation.\n",
        "\n",
        "In this case, instead of a PDF like above, the PDF is composed of fractional contributions to some number of components.  This is template fitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "igiLXwNMRxe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a direct example from [this paper](https://arxiv.org/pdf/2204.13530).  Here there are jets coming from three kinds of quarks (light, charm and bottom) and a template for what kind of $p_{T,rel}$ distribution each type of quark should produce.  The measured data are fitted to a PDF based on the template:\n",
        "\n"
      ],
      "metadata": {
        "id": "3nz8xDepTCgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://drive.google.com/uc?id=1RiLsQdJ9BpHVaSUdpFkp5QuO-bF2XvwC)\n"
      ],
      "metadata": {
        "id": "tWqZREg-UlmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, above there are three templates but only two fit parameters because the total integral of what is being fit is fixed."
      ],
      "metadata": {
        "id": "MKFJhhoIdaaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_formats = ['svg']\n",
        "%pip install iminuit\n",
        "from iminuit import Minuit\n",
        "from iminuit.cost import poisson_chi2, Template\n",
        "import numpy as np\n",
        "from scipy.stats import norm, truncexpon\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "from scipy.stats import expon\n",
        "from scipy.stats import norm\n",
        "\n"
      ],
      "metadata": {
        "id": "eEh4QYy6Zr90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0nHYnPguZx-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bins = 20\n",
        "xe = np.linspace(0, 6, bins + 1)\n",
        "\n",
        "data = np.append(expon.rvs(size=100),norm.rvs(size=100, loc = 3))\n",
        "print(np.shape(data))\n",
        "norm_data = norm.rvs(size=50, loc=3)\n",
        "expon_data = expon.rvs(size=50)\n",
        "data_hist, edges = np.histogram(data,xe)\n",
        "print(np.shape(data_hist))\n",
        "\n",
        "norm_template, edges = np.histogram(norm_data,xe)\n",
        "expon_template, edges = np.histogram(expon_data,xe)\n",
        "template = np.array([norm_template, expon_template])\n",
        "print(np.shape(template))"
      ],
      "metadata": {
        "id": "z3j5Ubw5ZyZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(10, 4), sharex=True)\n",
        "ax[0].stairs(data_hist, xe, fill=True)\n",
        "ax[1].stairs(norm_template, xe, fill=True)\n",
        "ax[2].stairs(expon_template, xe, fill=True)"
      ],
      "metadata": {
        "id": "RkZbGkLwaG-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(yields):\n",
        "    mu = 0\n",
        "    for y, c in zip(yields, template):\n",
        "        mu += y * c / np.sum(c)\n",
        "    r = poisson_chi2(data_hist, mu)\n",
        "    return r\n",
        "\n",
        "\n",
        "cost.errordef = Minuit.LEAST_SQUARES\n",
        "cost.ndata = np.prod(data_hist.shape)\n",
        "\n",
        "starts = np.ones(2)\n",
        "m = Minuit(cost, starts)\n",
        "m.limits = (0, None)\n",
        "m.migrad()\n",
        "m.hesse()"
      ],
      "metadata": {
        "id": "byUm2N-xaqjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The precision of the templates affects the precision of the fit.  "
      ],
      "metadata": {
        "id": "qMvAyh2ScgxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = 1000\n",
        "rng = np.random.default_rng(1)\n",
        "pars = []\n",
        "for ib in range(b):\n",
        "    ti = rng.poisson(template)\n",
        "    #vary the template to bootstrap the uncertainty\n",
        "\n",
        "    def cost(yields):\n",
        "        mu = 0\n",
        "        for y, c in zip(yields, ti):\n",
        "            mu += y * c / np.sum(c)\n",
        "        r = poisson_chi2(data_hist, mu)\n",
        "        return r\n",
        "\n",
        "    mi = Minuit(cost, m.values[:])\n",
        "    mi.errordef = Minuit.LEAST_SQUARES\n",
        "    mi.limits = (0, None)\n",
        "    mi.strategy = 0\n",
        "    mi.migrad()\n",
        "    assert mi.valid\n",
        "    pars.append(mi.values[:])\n",
        "\n",
        "cov2 = np.cov(np.transpose(pars), ddof=1)"
      ],
      "metadata": {
        "id": "LLW09tQDduV7"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov1 = m.covariance\n",
        "\n",
        "for title, cov in zip((\"data\", \"bootstrap\", \"fit+bootstrap\"), (cov1, cov2, cov1 + cov2)):\n",
        "    print(title)\n",
        "    for label, p, e in zip((\"b\", \"s\"), m.values, np.diag(cov) ** 0.5):\n",
        "        print(f\"  {label} {p:.0f} +- {e:.0f}\")\n",
        "    print(f\"  correlation {cov[0, 1] / np.prod(np.diag(cov)) ** 0.5:.2f}\")"
      ],
      "metadata": {
        "id": "i78-SF76dy2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}