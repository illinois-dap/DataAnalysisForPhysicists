{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SertjcIlqxT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import gauss\n",
        "from random import random\n",
        "from numpy.linalg import inv\n",
        "!pip install pyunfold\n",
        "from pyunfold import iterative_unfold\n",
        "from pyunfold.callbacks import Logger\n",
        "import matplotlib.cm as cm\n",
        "import scipy as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response, Unfolding & Convolution"
      ],
      "metadata": {
        "id": "UAu3EZdDaydb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response\n",
        "\n",
        "Your measurement isn't perfect.  For example:\n",
        "*  there is a finite resolution to the detector.  Often this will take the place of a Gaussian (or Gaussian-like) smearing around the true value.\n",
        "*  your measurement could be systematically off in scale (e.g. your ruler is 0.5cm too long)\n",
        "*  you can have an inefficiency in your detector either from the finite apature or some random loss.\n",
        "\n",
        "These effects all go into the measurement *response*.\n",
        "\n",
        "Every measurement suffers from this to some degree.  The issue is that experiments with different responses\n",
        "can leave to different measurements.  \n",
        "\n",
        "Mathetmatically:\n",
        "$$\\vec{M} = \\bf{R} \\vec{T}$$  If two experiments have\n",
        "different responses $\\bf{R_a}$ and $\\bf{R_b}$ but\n",
        "are measuring the same quantity $\\vec{T}$, then\n",
        "$\\vec{M_a}$ and $\\vec{M_b}$ will be different.\n",
        "\n",
        "$\\bf{R}$ is generally a matrix and it can contain:\n",
        "- inefficency of the detector\n",
        "- the inherent resolution of the measurement process\n",
        "- other effects\n",
        "\n"
      ],
      "metadata": {
        "id": "C0DXD8HLYFMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining the response\n",
        "\n",
        "The main way in which experimenters determine the response of a measurement is through simulations.  Here\n",
        "the simulations model the interaction of a simulated singal (similar to what is expected for the true values) are input into a model of the detector and the\n",
        "expected output is given.  This process is a *Monte Carlo*.\n",
        "\n",
        "This process can be incredibly detailed and usually requires more computing power than analyzing the data itself.  In many parts of physics/medicine, [Geant](https://geant4.web.cern.ch/) is used.  This simulates\n",
        "the passage of particles and radiation through matter based on decades of measurements for specific materials and energies.  Other examples are various ray tracking programs for optics.  From this, it is possible to determine the response matrix.\n",
        "\n",
        "Anything requiring a simulation is of course only as good as the simulation itself and all of these tools are imperfect to some degree.  Usually experimenters try to do as many checks as possible of the simulation using data itself.  \n",
        "\n",
        "(This process of generating the response from a given model is actually a *convolution* which we'll discuss at the end.)\n"
      ],
      "metadata": {
        "id": "OCt__5Q_YK3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporating the Response into Interpretation\n",
        "\n",
        "If the $\\bf{R}$ and $\\vec{M}$ are known then $\\vec{T}$\n",
        "is constained.  There is a choice as to whether to report $\\bf{R}$ and $\\vec{M}$ or to try to constrain $\\vec{T}$ itself.  The advantage of reporting $\\bf{R}$ and $\\vec{M}$ is the simplicity.  The disadvantage, clearly, is not reporting the actual quantity of interest $\\vec{T}$.  \n",
        "\n",
        "However, getting to $\\vec{T}$ can be a complicated process, as we'll see below.\n",
        "\n",
        "The most obvious way to get $\\vec{T}$ is to invert $\\bf{R}$ to get:\n",
        "$$\\bf{R^{-1}}\\vec{M} = \\bf{R^{-1}}\\bf{R} \\vec{T}$$\n",
        "$$\\bf{R^{-1}}\\vec{M} =  \\vec{T}$$\n",
        "\n",
        "We'll consider that as a first step, and then discuss *unfolding* which is a more stable proceedure to get\n",
        "$\\vec{T}$ from $\\bf{R}$ and $\\vec{M}$.\n",
        "\n",
        "First we'll generate samples, *num_samples*, according to a distribution.  This is what nature made for us."
      ],
      "metadata": {
        "id": "vjJo3TWWYPJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to have *four* samples here:\n",
        "\n",
        "Two are based on reality:\n",
        "- **data_true** this is nature.  It's what would be measured in a perfect detector.  This is unknown to you.\n",
        "- **data_reco** this is what we actually measure.  It's the dataset you have.\n",
        "\n",
        "The next two are based on a model:\n",
        "- **MC_true** this is the model for your simulation.  It is based only on your model and not on your detector.\n",
        "- **MC_reco** this is what your model would give in your detector if it were true.\n",
        "\n",
        "The goal is to use *MC_true* and *MC_reco* to unfold *data_reco* to *data_unfolded* that is your best understanding of *data_true*.\n",
        "\n",
        "All of the above are values themselves, when we put them in histograms, we'll append \"_hist\" to the name."
      ],
      "metadata": {
        "id": "eqQpC3f_QGiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_context(context='poster')\n",
        "plt.rcParams['figure.figsize'] = (10, 8)\n",
        "plt.rcParams['lines.markeredgewidth'] = 2\n",
        "\n",
        "hrange = 2\n",
        "nbins = 51\n",
        "num_samples = int(1e4)\n",
        "#true_samples = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n",
        "MC_true = np.random.exponential(scale=1, size=num_samples)\n",
        "data_true = np.random.exponential(scale=1, size=num_samples)\n",
        "\n",
        "bins = np.linspace(0, hrange, nbins)\n",
        "num_bins = len(bins) - 1\n",
        "data_true_hist, _ = np.histogram(data_true, bins=bins)\n",
        "MC_true_hist, _ = np.histogram(MC_true, bins=bins)\n",
        "fig, ax = plt.subplots()\n",
        "ax.step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='True distribution')\n",
        "ax.set(xlabel='X bins', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M8VyY7LZmLoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's mess up the truth.  Instead of a detailed Monte Carlo in this case we will just add noise via a shift in the mean with a Gaussian smearing.  This is what we think our detector actually measures given these truth samples."
      ],
      "metadata": {
        "id": "1kR_dfgmYS2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_width = 0.5\n",
        "noise_shift = 1.0\n",
        "random_noise = np.random.normal(loc=noise_shift, scale=noise_width, size=num_samples)\n",
        "MC_reco = MC_true * random_noise\n",
        "MC_reco_hist, _ = np.histogram(MC_reco, bins=bins)\n",
        "fig, ax = plt.subplots()\n",
        "ax.step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='MC True')\n",
        "ax.step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='MC Reco')\n",
        "ax.set(xlabel='X bins', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Db566iOEmQZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the response matrix (*response_hist*).  This is a 2D histogram that contains the correlation between each point in MC_true and MC_reco.\n",
        "\n",
        "The software requires an efficiency histogram.  In our case, we'll assume perfect efficiency, so it's just a histogram of ones."
      ],
      "metadata": {
        "id": "2SAFd0nESy_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MC_reco_err_hist = np.sqrt(MC_reco_hist)\n",
        "efficiencies = np.ones_like(MC_reco_hist, dtype=float)\n",
        "efficiencies\n",
        "efficiencies_err = np.full_like(efficiencies, 0.0, dtype=float)\n",
        "efficiencies_err\n",
        "response_hist, _, _ = np.histogram2d(MC_reco, MC_true, bins=bins)\n",
        "response_hist_err = np.sqrt(response_hist)"
      ],
      "metadata": {
        "id": "gH5aU3PImLMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to require each *column* (truth bin) to sum up to the efficiency value (here we've just taken all the efficiencies to be unity so all the columns will add up to one).  This is equivalent to saying that every truth value will be measured somewhere."
      ],
      "metadata": {
        "id": "7isaxC2UYY3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_sums = response_hist.sum(axis=0)\n",
        "normalization_factor = efficiencies / column_sums\n",
        "\n",
        "response = response_hist * normalization_factor\n",
        "response_err = response_hist_err * normalization_factor\n",
        "#checking...\n",
        "response.sum(axis=0)\n",
        "inv_response_hist = inv(response)\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(response, origin='lower')\n",
        "cbar = plt.colorbar(im, label='$P(E_i|C_{\\mu})$')\n",
        "ax.set(xlabel='MC_true', ylabel='MC_reco',\n",
        "       title='Normalized response matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p4Vc8nbxmBwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have the response matrix and it's inverse so we can try:\n",
        "$$\\bf{R^{-1}}\\vec{M} =  \\vec{T}$$\n",
        "\n",
        "Let's just try this only on the MC_* sample.  Can we recover MC_true by this process?"
      ],
      "metadata": {
        "id": "RG1rTrmZY9YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MC_reco_inv = np.matmul(inv_response_hist,MC_reco_hist)\n"
      ],
      "metadata": {
        "id": "95DwO1IaZALR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='MC_true')\n",
        "ax.step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='MC_reco')\n",
        "ax.step(np.arange(num_bins), MC_reco_inv, where='mid', lw=3,\n",
        "        alpha=0.7, label='from matrix inversion')\n",
        "ax.set(xlabel='X bins', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ToJAks5QZC_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Well that didn't work well. The green curve looks vaguely like the original distribution but it has far too many fluctuations to be useful in any way. Why? Well, let's take a look at this inverse matrix. What does it look like when we just let the truth equal the observations? It's easy to go back and turn the noise down to zero. Seeing that then the inversion works is a trivial but important check.\n",
        "\n",
        "(Also, run the process of generating the \"observed\" points that go into the response matrix multiple times; you will see that the inverse of the response matrix changes noticably depending on the particular random number set.)\n",
        "\n",
        "Finally, the matrix inversion method requires the response to be a square matrix; from the physics point of view, the response can be square, but need not be.\n"
      ],
      "metadata": {
        "id": "p2LOdVvrZGKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(inv_response_hist, origin='lower')\n",
        "cbar = plt.colorbar(im, label='')\n",
        "ax.set(xlabel='Cause bins', ylabel='Effect bins',\n",
        "       title='Inverse of response matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gqwk2QhVZIWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unfolding\n",
        "\n",
        "So where do we go from here?  The basic idea is *unfolding*.  This improves on the matrix inversion by including some additional information.  Usually there is a preference for a smooth final distribution.  This usually makes sense from a physical point of view but there are some clear dangers here.\n",
        "\n",
        "We're going to discuss *Iterative Bayesian Unfolding*.  This is based on Bayes' Theorem (go back to the beginning when we discussed the basics of probability).  This method is documented [here](https://inspirehep.net/literature/374574).  This is used extensively at the LHC, neutrino detectors, DM searches...\n",
        "\n",
        "Let's start with Bayes theorem:\n",
        "$$ P(C_i|E) = \\frac{P(E|C_i) P(C_i)}{\\sum_i P(E|C_i) P(C_i)}$$\n",
        "where the $C_i$ are each independent causes which can produce a single effect $E$.\n",
        "Here the conditional probability of cause $C_i$ to produce the effect $E$ is $P(E|C_i)$ and the initial\n",
        "probability of causes is $P(C_i)$.\n",
        "\n",
        "One observes some number of effects $E$, $n(E)$ and one\n",
        "wants to know the distribution of causes, $n(C_i)$ that causes those effects:\n",
        "$$\\hat{n}(C_i) = n(E) P(C_i|E) $$\n",
        "Most measurements are complicated and procues a number of effects,\n",
        "so for each effect $j$, Bayes theorem applies:\n",
        "$$P(C_i|E_j) = \\frac{P(E_j|C_i)P_{a}(C_i)}{n(E)}$$\n",
        "$$P(C_i|E_j) = \\frac{P(E_j|C_i)P_{a}(C_i)}{\\sum_lP(E_j|C_l)P_a(C_l)}$$\n",
        "\n",
        "Given a *guess* at $P_{a}(C_i)$ (e.g. the *prior*) and our understanding from simulation of how the pdf for each effect bin in terms of the cause bin, $P(E_j|C_i)$, is we can get a constraint on $P(C_i|E_j)$ and then we have both terms on the rhs of:\n",
        "$$\\hat{n}(C_i) = n(E) P(C_i|E) $$\n",
        "\n",
        "Then we iterate.  We calcuate $\\hat{n}(C_i)$ based on the intial guess and then use, those values as the new $P_a(C_i)$.  We iterate as many times as necessary until $\\hat{n}(C_i)$ is stable from one iteration to the next (we'll discuss how to make this qualitative statement quantitative below)."
      ],
      "metadata": {
        "id": "v_Jv4M9sZQgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's just try this on the MC itself and verify that we can recover MC_true_hist by unfolding MC_reco_hist.\n",
        "\n",
        "We have everything we need for that, except a criteria for stopping the iteration."
      ],
      "metadata": {
        "id": "uqgvetwRV1pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_values = [0.0001]\n",
        "print(ts_values)\n",
        "unfolded_results = []\n",
        "closure = []\n",
        "closure_err = []\n",
        "for i in range(1):\n",
        "  tmp_results = iterative_unfold(data=MC_reco_hist,\n",
        "                                    data_err=MC_reco_err_hist,\n",
        "                                    response=response,\n",
        "                                    response_err=response_err,\n",
        "                                    efficiencies=efficiencies,\n",
        "                                    ts_stopping=ts_values[i],\n",
        "                                    #ts='rmd',\n",
        "                                    efficiencies_err=efficiencies_err,\n",
        "                                    callbacks=[Logger()])\n",
        "  unfolded_results.append(tmp_results)\n",
        "  tmp_closure = tmp_results['unfolded'] / MC_true_hist\n",
        "  tmp_closure_err = tmp_results['sys_err'] / MC_true_hist\n",
        "  closure.append(tmp_closure)\n",
        "  closure_err.append(tmp_closure_err)\n",
        "\n",
        "print(len(unfolded_results), len(closure_err))\n",
        "\n"
      ],
      "metadata": {
        "id": "H0RGawogmA6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = cm.rainbow(np.linspace(0, 1, len(ts_values)))\n",
        "print(type(colors))\n",
        "fig, ax = plt.subplots(1,2,figsize=(15, 10))\n",
        "ax[0].step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='True distribution')\n",
        "ax[0].step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='Observed distribution')\n",
        "for i in range(len(ts_values)):\n",
        "  ax[0].errorbar(np.arange(num_bins), unfolded_results[i]['unfolded'],\n",
        "            yerr=unfolded_results[i]['sys_err'],\n",
        "            alpha=0.7,\n",
        "            elinewidth=3,\n",
        "            capsize=4,\n",
        "                 color=colors[i],\n",
        "            ls='None', marker='.', ms=10,\n",
        "            label='Bayesian unf. dist.')\n",
        "\n",
        "  ax[0].set(xlabel='X bins', ylabel='Counts')\n",
        "  ax[1].errorbar(np.arange(num_bins),closure[i], yerr=closure_err[i], color=colors[i])\n",
        "  ax[1].set(xlabel='X bins', ylabel='unfolded / truth')\n",
        "plt.legend(loc='lower center')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mJSYpAJavT94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Priors\n",
        "\n",
        "As with anything Bayesian, the selection of the prior central.  \n",
        "Choosing an appropriate prior affects the time to reach a the stopping condition.  In PyUnfold, the default prior is a uniform distribution.  Depending on the shape of the distribution and the knowledge you have about what the truth should be, this might or might not be an optimal choice.  \n",
        "\n",
        "In the example above, the truth distribution falls like an exponential.  There are many cases where one could know that the distribution should be steeply falling like that and that might be a better place to start.  Here we'll set the prior to an exponential but not with the same slope as the real distribution."
      ],
      "metadata": {
        "id": "ZUDFLfoWYM5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "alt_prior = np.random.exponential(scale=0.9, size=num_samples)\n",
        "\n",
        "alt_prior_hist, _ = np.histogram(alt_prior, bins=bins, density=True)\n",
        "alt_prior_hist /= np.sum(alt_prior_hist)\n"
      ],
      "metadata": {
        "id": "5AMuwaYW2tsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_values = [0.1, 0.01, 0.001, 0.0001]\n",
        "alt_unfolded_results = []\n",
        "alt_closure = []\n",
        "alt_closure_err = []\n",
        "for i in range(4):\n",
        "  tmp_results = iterative_unfold(data=MC_reco_hist,\n",
        "                                    data_err=MC_reco_err_hist,\n",
        "                                    response=response,\n",
        "                                    response_err=response_err,\n",
        "                                    efficiencies=efficiencies,\n",
        "                                    ts_stopping=ts_values[i],\n",
        "                                    prior=alt_prior_hist,\n",
        "                                    #ts='rmd',\n",
        "                                    efficiencies_err=efficiencies_err,\n",
        "                                    callbacks=[Logger()])\n",
        "  alt_unfolded_results.append(tmp_results)\n",
        "  tmp_closure = tmp_results['unfolded'] / MC_true_hist\n",
        "  tmp_closure_err = tmp_results['sys_err'] / MC_true_hist\n",
        "  alt_closure.append(tmp_closure)\n",
        "  alt_closure_err.append(tmp_closure_err)"
      ],
      "metadata": {
        "id": "zdMXw37j3cgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = cm.rainbow(np.linspace(0, 1, len(ts_values)))\n",
        "print(type(colors))\n",
        "fig, ax = plt.subplots(1,2,figsize=(15, 10))\n",
        "ax[0].step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='True distribution')\n",
        "ax[0].step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='Observed distribution')\n",
        "for i in range(len(ts_values)):\n",
        "  labelwords = 'unfolded, stop: ' + str(ts_values[i])\n",
        "  ax[0].errorbar(np.arange(num_bins), alt_unfolded_results[i]['unfolded'],\n",
        "            yerr=alt_unfolded_results[i]['sys_err'],\n",
        "            alpha=0.5,\n",
        "            elinewidth=3,\n",
        "            capsize=4,\n",
        "                 color=colors[i],\n",
        "            ls='None', marker='.', ms=10,\n",
        "            label=labelwords)\n",
        "\n",
        "  ax[0].set(xlabel='X bins', ylabel='Counts')\n",
        "  ax[1].errorbar(np.arange(num_bins),alt_closure[i], yerr=alt_closure_err[i], color=colors[i])\n",
        "  ax[1].set(xlabel='X bins', ylabel='unfolded / truth')\n",
        "fig.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6sr0WDHd4Isc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does converge faster in that case which improves the unfolding uncertainties."
      ],
      "metadata": {
        "id": "eDo0h7H14uRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uncertainties\n",
        "\n",
        "Statistical uncertainties come from:\n",
        "- uncertainties in the data\n",
        "- limited size of the MC sample\n",
        "In both cases it is usually the case that bootstrapping the statistical uncertainties is the most robust method of determining them.\n",
        "\n",
        "Systematic uncertainties largely come from the choice of the prior distribution.  The choice of how to handle this depends on how much the unfolding is doing (how different are MC_reco and MC_true?) and what is known about the prior."
      ],
      "metadata": {
        "id": "PhAwn6GtvH-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to set the stopping condition?\n",
        "\n",
        "There is a trade off.  The statistical uncertainties (correlated between the unfolded points) grow with every iteration, but unfolding further does allow more time to reach a stable working point.  Usually,\n",
        "one takes a minimum in:\n",
        "$$\\sum_{bins} \\left(\\sigma^2_{stat} + \\sigma^2_{conv}\\right)$$\n",
        "where $\\sigma_{stat}$ is statistical uncertainty from a bin and $\\sigma_{conv}$ is the difference between the unfolded results for $n$ and $n-1$ iterations.\n",
        "\n",
        "In another way of doing it, Pyunfold implements a variety of test statistics to determine the stopping condition.\n",
        "\n",
        "One wants to iterate as few times as necessary because over iterating causes large fluctuations/uncertainties.\n",
        "\n"
      ],
      "metadata": {
        "id": "XGldIwkzV1b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we've done so far is set up a nice example but we have been unfolding our MC_reco sample with a response matrix generated from MC_reco and MC_true.  No where has anything like a real measurement come in.  \n",
        "\n",
        "Here we'll go back to the data_true sample we generated above and generate data_reco.  This sample was generated independently from MC_true and so this is the situation that one would have with a\n",
        "real measurement scenario.  We will also generate an independent noise sample (but with the same parameters as above).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ayZVySGZgba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_noise2 = np.random.normal(loc=noise_shift, scale=noise_width, size=num_samples)\n",
        "data_reco = data_true * random_noise2\n",
        "data_reco_hist, _ = np.histogram(data_reco, bins=bins)\n",
        "data_reco_err_hist = np.sqrt(data_reco_hist)"
      ],
      "metadata": {
        "id": "UYStb_JD5vnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_unfolded_results = []\n",
        "data_closure = []\n",
        "data_closure_err = []\n",
        "for i in range(len(ts_values)):\n",
        "  tmp_results = iterative_unfold(data=data_reco_hist,\n",
        "                                    data_err=data_reco_err_hist,\n",
        "                                    response=response,\n",
        "                                    response_err=response_err,\n",
        "                                    efficiencies=efficiencies,\n",
        "                                    ts_stopping=ts_values[i],\n",
        "                                    prior=alt_prior_hist,\n",
        "                                    #ts='rmd',\n",
        "                                    efficiencies_err=efficiencies_err,\n",
        "                                    callbacks=[Logger()])\n",
        "  data_unfolded_results.append(tmp_results)\n",
        "  tmp_closure = tmp_results['unfolded'] / data_true_hist\n",
        "  tmp_closure_err = tmp_results['sys_err'] / data_true_hist\n",
        "  data_closure.append(tmp_closure)\n",
        "  data_closure_err.append(tmp_closure_err)"
      ],
      "metadata": {
        "id": "5BAdhN7W6IRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = cm.rainbow(np.linspace(0, 1, len(ts_values)))\n",
        "print(type(colors))\n",
        "fig, ax = plt.subplots(1,2,figsize=(15, 10))\n",
        "ax[0].step(np.arange(num_bins), data_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='data True distribution')\n",
        "ax[0].step(np.arange(num_bins), data_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='data reco distribution')\n",
        "for i in range(len(ts_values)):\n",
        "  labelwords = 'unfolded, stop: ' + str(ts_values[i])\n",
        "  ax[0].errorbar(np.arange(num_bins), data_unfolded_results[i]['unfolded'],\n",
        "            yerr=data_unfolded_results[i]['sys_err'],\n",
        "            alpha=0.7,\n",
        "            elinewidth=3,\n",
        "            capsize=4,\n",
        "                 color=colors[i],\n",
        "            ls='None', marker='.', ms=10,\n",
        "            label=labelwords)\n",
        "\n",
        "  ax[0].set(xlabel='X bins', ylabel='Counts')\n",
        "  ax[1].errorbar(np.arange(num_bins),data_closure[i], yerr=data_closure_err[i], color=colors[i])\n",
        "  ax[1].set(xlabel='X bins', ylabel='unfolded / truth')\n",
        "fig.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5B86dXA56u-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More than one dimension\n",
        "\n",
        "- It's straightforward to conceptualize a single dimensional unfolding.  The same procedure works in multiple dimensions, though it becomes more difficult to visualize and understand the uncertainty on a multi-dimensional unfolding.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IHzhjLZmwbYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution\n",
        "\n",
        "A convolution is an operation that takes place on two functions, $f$ and $g$, producing a third function $f * g$:\n",
        "$$(f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau$$\n",
        "Intuitively, this involves flipping one of the functions and sliding it past the other.  The value of the convlution at a particular point is then the integral of the overlap of the two functions.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1olgQ6PWsmguah8NHyilyVK3R9Sl4nt6Q) (image from wikipedia)\n",
        "\n",
        "There are a few properties of convolutions that are useful to know:\n",
        "- $(f * g) = (g *f)$, communtativity\n",
        "- $f * (g * h) =  (f * g) * h$, associativity\n",
        "- $f * (g + h) = f * g + f * h$, distributivity\n",
        "- $ f * \\delta = f$\n",
        "\n",
        "Convolutions show up in lots of places, particularly signal processing, responses, physics, etc.  This is what is going on in kernel density estimation as well (from earlier in the semester).\n",
        "\n",
        "Of course there are multiple implementations of convolutions in python (we'll use the scipy one) but it is easy to lose intuition of what's going on without working through some simple examples.\n",
        "\n",
        "Here's a simple way to think abou a discrete convolution.  Let's say that we have $f = [1 2 3 4]$ and $g = [1 2]$.  In order to evaluate the convolution, we flip $g$ and slide it across $f$.  At each step, we\n",
        "take the product of the element of $f$ and the element of $g$ that are lined up.  Those elements are summed together to create an element of the convolution.\n",
        "![](https://drive.google.com/uc?export=view&id=1BBxKcLTPmZh1gWmS68dJUY9_eDZOxOmg)\n",
        "\n",
        "\n",
        "\n",
        "Let's verify that this is the same as one gets using scipy's convolution function..."
      ],
      "metadata": {
        "id": "c1yB4Tfj1apN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import signal\n",
        "sig = ([1,2,3,4])\n",
        "win = ([1,2])\n",
        "\n",
        "convolution = signal.convolve(sig, win, mode='full')\n",
        "print(convolution)\n"
      ],
      "metadata": {
        "id": "cBux0oQZ3DPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can more over to more complicated functions.  Here is a $\\delta$ function convoluted with a Gaussian."
      ],
      "metadata": {
        "id": "u3TGKvpX3MZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sig = np.repeat([0],100)\n",
        "sig = np.append(sig,np.repeat([1],1))\n",
        "sig = np.append(sig,np.repeat([0],100))\n",
        "win = signal.windows.gaussian(60,10,)\n",
        "filtered = signal.convolve(sig, win, mode='same') / sum(win)\n",
        "\n",
        "fig, (ax_orig, ax_win, ax_filt) = plt.subplots(3, 1, sharex=True)\n",
        "ax_orig.plot(sig)\n",
        "ax_orig.set_title('Original pulse')\n",
        "ax_orig.margins(0, 0.1)\n",
        "ax_win.plot(win)\n",
        "ax_win.set_title('Filter impulse response')\n",
        "ax_win.margins(0, 0.1)\n",
        "ax_filt.plot(filtered)\n",
        "ax_filt.set_title('Filtered signal')\n",
        "ax_filt.margins(0, 0.1)\n",
        "fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "ihV0kIB13PTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadening this out to a wider flat distribution..."
      ],
      "metadata": {
        "id": "KjE18NJJ3n9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sig = np.repeat([0],100)\n",
        "sig = np.append(sig,np.repeat([1],10))\n",
        "sig = np.append(sig,np.repeat([0],100))\n",
        "\n",
        "win = signal.windows.gaussian(60,10,)\n",
        "\n",
        "filtered = signal.convolve(sig, win, mode='same') / sum(win)\n",
        "\n",
        "fig, (ax_orig, ax_win, ax_filt) = plt.subplots(3, 1, sharex=True)\n",
        "ax_orig.plot(sig)\n",
        "ax_orig.set_title('Original pulse')\n",
        "ax_orig.margins(0, 0.1)\n",
        "ax_win.plot(win)\n",
        "ax_win.set_title('Filter impulse response')\n",
        "ax_win.margins(0, 0.1)\n",
        "ax_filt.plot(filtered)\n",
        "ax_filt.set_title('Filtered signal')\n",
        "ax_filt.margins(0, 0.1)\n",
        "fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "bEzEsUGP3lg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig = np.repeat([0., 1., 0.], 100)\n",
        "win = signal.windows.gaussian(60,10,)\n",
        "\n",
        "\n",
        "filtered = signal.convolve(sig, win, mode='same') / sum(win)\n",
        "fig, (ax_orig, ax_win, ax_filt) = plt.subplots(3, 1, sharex=True)\n",
        "ax_orig.plot(sig)\n",
        "ax_orig.set_title('Original pulse')\n",
        "ax_orig.margins(0, 0.1)\n",
        "ax_win.plot(win)\n",
        "ax_win.set_title('Filter impulse response')\n",
        "ax_win.margins(0, 0.1)\n",
        "ax_filt.plot(filtered)\n",
        "ax_filt.set_title('Filtered signal')\n",
        "ax_filt.margins(0, 0.1)\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "niiF3VEE-Tmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's worth knowing that a Gaussian convoluted with a Gaussian remains a Gaussian."
      ],
      "metadata": {
        "id": "9LURjUSA00OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "x = np.linspace(-4, 4, 100)\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "sig = stats.norm.pdf(x, mean, std_dev)\n",
        "win = signal.windows.gaussian(20,10,)\n",
        "\n",
        "\n",
        "filtered = signal.convolve(sig, win, mode='same') / sum(win)\n",
        "fig, (ax_orig, ax_win, ax_filt) = plt.subplots(3, 1, sharex=True)\n",
        "ax_orig.plot(sig)\n",
        "ax_orig.set_title('Original pulse')\n",
        "ax_orig.margins(0, 0.1)\n",
        "ax_win.plot(win)\n",
        "ax_win.set_title('Filter impulse response')\n",
        "ax_win.margins(0, 0.1)\n",
        "ax_filt.plot(filtered)\n",
        "ax_filt.set_title('Filtered signal')\n",
        "ax_filt.margins(0, 0.1)\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PYOGGjHp0Gmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw that convolving something with sharp edges with a Gaussian smooths out the edges, as we expect.  One use for this is in image processing.  Here we'll create a 2D Gaussian and blur an image."
      ],
      "metadata": {
        "id": "Sk0Hr1Atar2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import subprocess\n",
        "\n",
        "def wget_data(url):\n",
        "    local_path='./'\n",
        "    subprocess.run([\"wget\", \"-nc\", \"-P\", local_path, url])\n"
      ],
      "metadata": {
        "id": "JrcDMVsHg8v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def locate_data(name, check_exists=True):\n",
        "    local_path='./'\n",
        "    path = os.path.join(local_path, name)\n",
        "    if check_exists and not os.path.exists(path):\n",
        "        raise RuxntimeError('No such data file: {}'.format(path))\n",
        "    return path"
      ],
      "metadata": {
        "id": "DKz1YKnfhMdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wget_data('https://raw.githubusercontent.com/illinois-dap/DataAnalysisForPhysicists/refs/heads/main/data/belt.png')\n",
        "locate_data('belt.png')"
      ],
      "metadata": {
        "id": "Qjyw7J9khDEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = plt.imread(\"./belt.png\")\n",
        "plt.figure()\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "B8I7TcWehPr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.linspace(-10, 10, 60)\n",
        "sigma2 = 20\n",
        "bump = np.exp(- t**2 /sigma2)\n",
        "bump /= np.sum(bump)\n",
        "kernel = bump[:, np.newaxis] * bump[np.newaxis, :]\n",
        "fig, ax = plt.subplots(1,2,figsize=(10, 5))\n",
        "ax[0].step(t, bump, where='mid', lw=3,\n",
        "        alpha=0.7)\n",
        "ax[0].set(xlabel='x', ylabel='amplitude')\n",
        "\n",
        "ax[1].imshow(kernel)\n",
        "plt.legend(loc='lower center')\n",
        "plt.show()\n",
        "#print(kernel)"
      ],
      "metadata": {
        "id": "syE6JBEKpqbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img3 = sp.signal.convolve(img, kernel[:, :, np.newaxis], mode=\"same\")\n",
        "plt.figure()\n",
        "plt.imshow(img3)"
      ],
      "metadata": {
        "id": "9jxcDniapulB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}