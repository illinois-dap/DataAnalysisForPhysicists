{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SertjcIlqxT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import gauss\n",
        "from random import random\n",
        "from numpy.linalg import inv\n",
        "!pip install pyunfold\n",
        "from pyunfold import iterative_unfold\n",
        "from pyunfold.callbacks import Logger\n",
        "import matplotlib.cm as cm\n",
        "import scipy as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unfolding"
      ],
      "metadata": {
        "id": "UAu3EZdDaydb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response\n",
        "\n",
        "Your measurement isn't perfect.  For example:\n",
        "*  there is a finite resolution to the detector.  Often this will take the place of a Gaussian (or Gaussian-like) smearing around the true value.\n",
        "*  your measurement could be systematically off in scale (e.g. your ruler is 0.5cm too long)\n",
        "*  you can have an inefficiency in your detector either from the finite apature or some random loss.\n",
        "\n",
        "These effects all go into the measurement *response*.\n",
        "\n",
        "Every measurement suffers from this to some degree.  The issue is that experiments with different responses\n",
        "can leave to different measurements.  \n",
        "\n",
        "Mathetmatically:\n",
        "$$\\vec{M} = \\bf{R} \\vec{T}$$  If two experiments have\n",
        "different responses $\\bf{R_a}$ and $\\bf{R_b}$ but\n",
        "are measuring the same quantity $\\vec{T}$, then\n",
        "$\\vec{M_a}$ and $\\vec{M_b}$ will be different.\n",
        "\n",
        "$\\bf{R}$ is generally a matrix and it can contain:\n",
        "- inefficency of the detector\n",
        "- the inherent resolution of the measurement process\n",
        "- other effects\n",
        "\n"
      ],
      "metadata": {
        "id": "C0DXD8HLYFMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining the response\n",
        "\n",
        "The main way in which experimenters determine the response of a measurement is through simulations.  Here\n",
        "the simulations model the interaction of a simulated singal (similar to what is expected for the true values) are input into a model of the detector and the\n",
        "expected output is given.  This process is a *Monte Carlo*.\n",
        "\n",
        "This process can be incredibly detailed and usually requires more computing power than analyzing the data itself.  In many parts of physics/medicine, [Geant](https://geant4.web.cern.ch/) is used.  This simulates\n",
        "the passage of particles and radiation through matter based on decades of measurements for specific materials and energies.  Other examples are various ray tracking programs for optics.  From this, it is possible to determine the response matrix.\n",
        "\n",
        "Anything requiring a simulation is of course only as good as the simulation itself and all of these tools are imperfect to some degree.  Usually experimenters try to do as many checks as possible of the simulation using data itself.  \n",
        "\n",
        "(This process of generating the response from a given model is actually a *convolution* which we'll discuss at the end.)\n"
      ],
      "metadata": {
        "id": "OCt__5Q_YK3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporating the Response into Interpretation\n",
        "\n",
        "If the $\\bf{R}$ and $\\vec{M}$ are known then $\\vec{T}$\n",
        "is constained.  There is a choice as to whether to report $\\bf{R}$ and $\\vec{M}$ or to try to constrain $\\vec{T}$ itself.  The advantage of reporting $\\bf{R}$ and $\\vec{M}$ is the simplicity.  The disadvantage, clearly, is not reporting the actual quantity of interest $\\vec{T}$.  \n",
        "\n",
        "However, getting to $\\vec{T}$ can be a complicated process, as we'll see below.\n",
        "\n",
        "The most obvious way to get $\\vec{T}$ is to invert $\\bf{R}$ to get:\n",
        "$$\\bf{R^{-1}}\\vec{M} = \\bf{R^{-1}}\\bf{R} \\vec{T}$$\n",
        "$$\\bf{R^{-1}}\\vec{M} =  \\vec{T}$$\n",
        "\n",
        "We'll consider that as a first step, and then discuss *unfolding* which is a more stable proceedure to get\n",
        "$\\vec{T}$ from $\\bf{R}$ and $\\vec{M}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "vjJo3TWWYPJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unfolding\n",
        "\n",
        "Unfolding is the process of obtaining $\\vec{T}$ from\n",
        "$\\vec{R}$ and $\\vec{M}$.  We'll first show the issues with the simplest methods and then demonstrate standard unfolding methods.  Finally, we'll discuss approaches which use machine learning to improve unfolding.\n",
        "\n",
        "We will use [pyunfold](https://jrbourbeau.github.io/pyunfold/) for the standard unfolding.  This package implements a Bayesian method described in [this paper](https://inspirehep.net/literature/374574) which is widely used at the LHC."
      ],
      "metadata": {
        "id": "pjLGGMVaqYWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a simple dataset and response\n",
        "\n",
        "We have two classes of points:\n",
        "- *data* are those which are from nature\n",
        "  - *data_true* ($\\vec{T}$) is what one is trying to measure\n",
        "  - *data_reco* ($\\vec{M}$) is what is measured in the detector\n",
        "- *MC* samples are necessary to determine $\\vec{R}$\n",
        "  - *MC_true* is a simulated signal\n",
        "  - *MC_reco* is the MC_true signal ran through your detector model\n",
        "\n",
        "All of this will be generated here but in a real setup the *data* would be related to the measurement.  \n",
        "\n",
        "The goal is to know *data_true* from a measurement of *data_reco* using information from *MC_true* and *MC_reco*."
      ],
      "metadata": {
        "id": "080jdEcfIn9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "All of the above are values themselves, when we put them in histograms, we'll append \"_hist\" to the name.\n",
        "\n",
        "For both *data_true* and *MC_true* I've chosen an exponential function (and the same exponential for both of them).  In real life, this would likely be significantly more complicated and it is unlikely that the MC model and the data itself would be drawn from exactly the same underlying distribution.\n",
        "\n",
        "The *data_true* and *MC_true* samples are statistically independent due to different seeds in the random number generator.\n",
        "\n",
        "I've also chosen to make 10 times more MC samples than data samples.  If possible it is generally better to have many more MC samples than data samples so that the statistical uncertainties are limited by the data rather than the MC.  "
      ],
      "metadata": {
        "id": "eqQpC3f_QGiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_context(context='poster')\n",
        "plt.rcParams['figure.figsize'] = (10, 8)\n",
        "plt.rcParams['lines.markeredgewidth'] = 2\n",
        "\n",
        "hrange = 5\n",
        "nbins = 51\n",
        "num_samples = int(1e4)\n",
        "MC_true = np.random.exponential(scale=1, size=num_samples)\n",
        "data_true = np.random.exponential(scale=1, size=10*num_samples)\n",
        "\n",
        "print(data_true[:5])\n",
        "print(MC_true[:5])\n",
        "\n",
        "\n",
        "bins = np.linspace(0, hrange, nbins)\n",
        "num_bins = len(bins) - 1\n",
        "data_true_hist, _ = np.histogram(data_true, bins=bins, range=(0,hrange))\n",
        "MC_true_hist, _ = np.histogram(MC_true, bins=bins, range=[0,hrange])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.step(bins[:nbins-1],MC_true_hist)\n",
        "ax.set(xlabel='x value', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M8VyY7LZmLoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling detector effects\n",
        "\n",
        "Now let's mess up the truth to mimic real detector effects.  \n",
        "\n",
        "Instead of a detailed Monte Carlo in this case we will just add noise via:\n",
        "- a shift in the mean (*noise_shift*) with a Gaussian smearing (with width *noise_width*).  \n",
        "\n",
        "There shouldn't be too many samples where the *random_noise* would be a negative number but we've added an absolute value just to make sure that all of the measured values remain non-negative.\n",
        "This isn't necessary from an unfolding point of view but it does keep the histograms from having negative x-axis values so it's just a bit simpler.\n",
        "\n",
        "This process gives what we think our detector actually measures given these truth samples."
      ],
      "metadata": {
        "id": "1kR_dfgmYS2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_width = 0.2\n",
        "noise_shift = 1.1\n",
        "random_noise = np.random.normal(loc=noise_shift, scale=noise_width, size=num_samples)\n",
        "random_noise = abs(random_noise) # here we're just going to ensure that all elements remain positive\n",
        "MC_reco = MC_true * random_noise\n",
        "MC_reco_hist, _ = np.histogram(MC_reco, bins=bins)\n",
        "fig, ax = plt.subplots()\n",
        "ax.step(bins[:nbins-1],MC_true_hist, label='MC True')\n",
        "ax.step(bins[:nbins-1],MC_reco_hist, label='MC Reco')\n",
        "ax.set(xlabel='x value', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Db566iOEmQZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MC_reco_hist and MC_true_hist histograms are clearly different but the MC_reco and MC_true should maintain their correlation line by line."
      ],
      "metadata": {
        "id": "QKbnMkVJvUIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MC_reco[:6])\n",
        "print(MC_true[:6])"
      ],
      "metadata": {
        "id": "mjSQW91Mx2-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the response matrix (*response_hist*).  This is a 2D histogram that contains the correlation between each element in MC_true and MC_reco.\n",
        "\n",
        "Additionally, the pyunfold software requires an efficiency histogram.  In our case, we'll assume perfect efficiency with no uncertainty on the efficiency, so it's just a histogram of ones and the efficiency error histogram is just a histogram of zeros."
      ],
      "metadata": {
        "id": "2SAFd0nESy_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MC_reco_err_hist = np.sqrt(MC_reco_hist)\n",
        "efficiencies = np.ones_like(MC_reco_hist, dtype=float)\n",
        "efficiencies\n",
        "efficiencies_err = np.full_like(efficiencies, 0.0, dtype=float)\n",
        "efficiencies_err\n",
        "response_hist, _, _ = np.histogram2d(MC_reco, MC_true, bins=bins) #this is the proper ordering for the axes\n",
        "response_hist_err = np.sqrt(response_hist)\n"
      ],
      "metadata": {
        "id": "gH5aU3PImLMV"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to require each *column* (truth bin) to sum up to the efficiency value (here we've just taken all the efficiencies to be unity so all the columns will add up to one).  This is equivalent to saying that every truth value will be measured somewhere.  \n",
        "\n",
        "We can slice up the response matrix to get a look at where the true values come from for a particular measured value."
      ],
      "metadata": {
        "id": "7isaxC2UYY3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slice = response_hist[5:6,1:50]\n",
        "fig, ax = plt.subplots()\n",
        "ax.step(bins[:nbins-2],np.reshape(slice,49), label='MC True for MC reco bin 5')\n",
        "ax.set(xlabel='MC true value', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l3JRll0K1-T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_sums = response_hist.sum(axis=0)\n",
        "normalization_factor = efficiencies / column_sums\n",
        "\n",
        "response = response_hist * normalization_factor\n",
        "response_err = response_hist_err * normalization_factor\n",
        "#checking...\n",
        "response.sum(axis=0)\n",
        "inv_response_hist = inv(response)\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(response, origin='lower')\n",
        "cbar = plt.colorbar(im, label='$P(E_i|C_{\\mu})$')\n",
        "ax.set(xlabel='MC_true bins', ylabel='MC_reco bins',\n",
        "       title='Normalized response matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p4Vc8nbxmBwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have the response matrix and it's inverse so we can try:\n",
        "$$\\bf{R^{-1}}\\vec{M} =  \\vec{T}$$\n",
        "\n",
        "Let's just try this only on the MC_* sample.  Can we recover MC_true by this process?"
      ],
      "metadata": {
        "id": "RG1rTrmZY9YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MC_reco_inv = np.matmul(inv_response_hist,MC_reco_hist)\n"
      ],
      "metadata": {
        "id": "95DwO1IaZALR"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='MC_true')\n",
        "ax.step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='MC_reco')\n",
        "ax.step(np.arange(num_bins), MC_reco_inv, where='mid', lw=3,\n",
        "        alpha=0.7, label='from matrix inversion')\n",
        "ax.set(xlabel='X bins', ylabel='Counts')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ToJAks5QZC_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Well that didn't work well. The green curve looks vaguely like the original distribution but it has far too many fluctuations to be useful in any way. Why? Well, let's take a look at this inverse matrix. What does it look like when we just let the truth equal the observations? It's easy to go back and turn the noise down to zero. Seeing that then the inversion works is a trivial but important check.\n",
        "\n",
        "(Also, run the process of generating the \"observed\" points that go into the response matrix multiple times; you will see that the inverse of the response matrix changes noticably depending on the particular random number set.)\n",
        "\n",
        "Finally, the matrix inversion method requires the response to be a square matrix; from the physics point of view, the response can be square, but need not be.\n"
      ],
      "metadata": {
        "id": "p2LOdVvrZGKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(inv_response_hist, origin='lower')\n",
        "cbar = plt.colorbar(im, label='')\n",
        "ax.set(xlabel='Cause bins', ylabel='Effect bins',\n",
        "       title='Inverse of response matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gqwk2QhVZIWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unfolding\n",
        "\n",
        "So where do we go from here?  The basic idea is *unfolding*.  This improves on the matrix inversion by including some additional information.  Usually there is a preference for a smooth final distribution.  This usually makes sense from a physical point of view but there are some clear dangers here.\n",
        "\n",
        "We're going to discuss *Iterative Bayesian Unfolding*.  This is based on Bayes' Theorem (go back to the beginning when we discussed the basics of probability).  This method is documented [here](https://inspirehep.net/literature/374574).  This is used extensively at the LHC, neutrino detectors, DM searches...\n",
        "\n",
        "Let's start with Bayes theorem:\n",
        "$$ P(C_i|E) = \\frac{P(E|C_i) P(C_i)}{\\sum_i P(E|C_i) P(C_i)}$$\n",
        "where the $C_i$ are each independent causes which can produce a single effect $E$.\n",
        "Here the conditional probability of cause $C_i$ to produce the effect $E$ is $P(E|C_i)$ and the initial\n",
        "probability of causes is $P(C_i)$.\n",
        "\n",
        "One observes some number of effects $E$, $n(E)$ and one\n",
        "wants to know the distribution of causes, $n(C_i)$ that causes those effects:\n",
        "$$\\hat{n}(C_i) = n(E) P(C_i|E) $$\n",
        "Most measurements are complicated and procudes a number of effects,\n",
        "so for each effect $j$, Bayes theorem applies:\n",
        "$$P(C_i|E_j) = \\frac{P(E_j|C_i)P_{a}(C_i)}{n(E)}$$\n",
        "$$P(C_i|E_j) = \\frac{P(E_j|C_i)P_{a}(C_i)}{\\sum_lP(E_j|C_l)P_a(C_l)}$$\n",
        "\n",
        "Given a *guess* at $P_{a}(C_i)$ (e.g. the *prior*) and our understanding from simulation of how the pdf for each effect bin in terms of the cause bin, $P(E_j|C_i)$, is we can get a constraint on $P(C_i|E_j)$ and then we have both terms on the rhs of:\n",
        "$$\\hat{n}(C_i) = n(E) P(C_i|E) $$\n",
        "\n",
        "Then we iterate.  We calcuate $\\hat{n}(C_i)$ based on the intial guess and then use, those values as the new $P_a(C_i)$.  We iterate as many times as necessary until $\\hat{n}(C_i)$ is stable from one iteration to the next (we'll discuss how to make this qualitative statement quantitative below)."
      ],
      "metadata": {
        "id": "v_Jv4M9sZQgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's just try this on the MC itself and verify that we can recover MC_true_hist by unfolding MC_reco_hist.\n",
        "\n",
        "We have everything we need for that, except a criteria for stopping the iteration.  This is always something that takes time to establish.  Here we'll just set an arbitrary value."
      ],
      "metadata": {
        "id": "uqgvetwRV1pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_values = [0.0001]\n",
        "print(ts_values)\n",
        "unfolded_results = []\n",
        "closure = []\n",
        "closure_err = []\n",
        "for i in range(1):\n",
        "  tmp_results = iterative_unfold(data=MC_reco_hist,\n",
        "                                    data_err=MC_reco_err_hist,\n",
        "                                    response=response,\n",
        "                                    response_err=response_err,\n",
        "                                    efficiencies=efficiencies,\n",
        "                                    ts_stopping=ts_values[i],\n",
        "                                    #ts='rmd',\n",
        "                                    efficiencies_err=efficiencies_err,\n",
        "                                    callbacks=[Logger()])\n",
        "  unfolded_results.append(tmp_results)\n",
        "  tmp_closure = tmp_results['unfolded'] / MC_true_hist\n",
        "  tmp_closure_err = tmp_results['sys_err'] / MC_true_hist\n",
        "  closure.append(tmp_closure)\n",
        "  closure_err.append(tmp_closure_err)\n",
        "\n",
        "print(len(unfolded_results), len(closure_err))\n",
        "\n"
      ],
      "metadata": {
        "id": "H0RGawogmA6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = cm.rainbow(np.linspace(0, 1, len(ts_values)))\n",
        "print(type(colors))\n",
        "fig, ax = plt.subplots(1,2,figsize=(15, 10))\n",
        "ax[0].step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='True distribution')\n",
        "ax[0].step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='Observed distribution')\n",
        "for i in range(len(ts_values)):\n",
        "  ax[0].errorbar(np.arange(num_bins), unfolded_results[i]['unfolded'],\n",
        "            yerr=unfolded_results[i]['sys_err'],\n",
        "            alpha=0.7,\n",
        "            elinewidth=3,\n",
        "            capsize=4,\n",
        "                 color=colors[i],\n",
        "            ls='None', marker='.', ms=10,\n",
        "            label='Bayesian unf. dist.')\n",
        "\n",
        "  ax[0].set(xlabel='X bins', ylabel='Counts')\n",
        "  ax[1].errorbar(np.arange(num_bins),closure[i], yerr=closure_err[i], color=colors[i])\n",
        "  ax[1].set(xlabel='X bins', ylabel='unfolded / truth')\n",
        "plt.legend(loc='lower center')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mJSYpAJavT94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Priors\n",
        "\n",
        "As with anything Bayesian, the selection of the prior central.  \n",
        "Choosing an appropriate prior affects the time to reach a the stopping condition.  In PyUnfold, the default prior is a uniform distribution.  Depending on the shape of the distribution and the knowledge you have about what the truth should be, this might or might not be an optimal choice.  \n",
        "\n",
        "In the example above, the truth distribution falls like an exponential.  There are many cases where one could know that the distribution should be steeply falling like that and that might be a better place to start.  Here we'll set the prior to an exponential but not with the same slope as the real distribution."
      ],
      "metadata": {
        "id": "ZUDFLfoWYM5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alt_prior = np.random.exponential(scale=0.9, size=num_samples)\n",
        "\n",
        "alt_prior_hist, _ = np.histogram(alt_prior, bins=bins, density=True)\n",
        "alt_prior_hist /= np.sum(alt_prior_hist)\n"
      ],
      "metadata": {
        "id": "5AMuwaYW2tsv"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_values = [0.1, 0.01, 0.001, 0.0001]\n",
        "alt_unfolded_results = []\n",
        "alt_closure = []\n",
        "alt_closure_err = []\n",
        "for i in range(4):\n",
        "  tmp_results = iterative_unfold(data=MC_reco_hist,\n",
        "                                    data_err=MC_reco_err_hist,\n",
        "                                    response=response,\n",
        "                                    response_err=response_err,\n",
        "                                    efficiencies=efficiencies,\n",
        "                                    ts_stopping=ts_values[i],\n",
        "                                    prior=alt_prior_hist,\n",
        "                                    #ts='rmd',\n",
        "                                    efficiencies_err=efficiencies_err,\n",
        "                                    callbacks=[Logger()])\n",
        "  alt_unfolded_results.append(tmp_results)\n",
        "  tmp_closure = tmp_results['unfolded'] / MC_true_hist\n",
        "  tmp_closure_err = tmp_results['sys_err'] / MC_true_hist\n",
        "  alt_closure.append(tmp_closure)\n",
        "  alt_closure_err.append(tmp_closure_err)"
      ],
      "metadata": {
        "id": "zdMXw37j3cgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = cm.rainbow(np.linspace(0, 1, len(ts_values)))\n",
        "print(type(colors))\n",
        "fig, ax = plt.subplots(1,2,figsize=(15, 10))\n",
        "ax[0].step(np.arange(num_bins), MC_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='True distribution')\n",
        "ax[0].step(np.arange(num_bins), MC_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='Observed distribution')\n",
        "for i in range(len(ts_values)):\n",
        "  labelwords = 'unfolded, stop: ' + str(ts_values[i])\n",
        "  ax[0].errorbar(np.arange(num_bins), alt_unfolded_results[i]['unfolded'],\n",
        "            yerr=alt_unfolded_results[i]['sys_err'],\n",
        "            alpha=0.5,\n",
        "            elinewidth=3,\n",
        "            capsize=4,\n",
        "                 color=colors[i],\n",
        "            ls='None', marker='.', ms=10,\n",
        "            label=labelwords)\n",
        "\n",
        "  ax[0].set(xlabel='X bins', ylabel='Counts')\n",
        "  ax[1].errorbar(np.arange(num_bins),alt_closure[i], yerr=alt_closure_err[i], color=colors[i])\n",
        "  ax[1].set(xlabel='X bins', ylabel='unfolded / truth')\n",
        "fig.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6sr0WDHd4Isc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does converge faster in that case which improves the unfolding uncertainties."
      ],
      "metadata": {
        "id": "eDo0h7H14uRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uncertainties\n",
        "\n",
        "Statistical uncertainties come from:\n",
        "- uncertainties in the data\n",
        "- limited size of the MC sample\n",
        "In both cases it is usually the case that bootstrapping the statistical uncertainties is the most robust method of determining them.\n",
        "\n",
        "Systematic uncertainties largely come from the choice of the prior distribution.  The choice of how to handle this depends on how much the unfolding is doing (how different are MC_reco and MC_true?) and what is known about the prior."
      ],
      "metadata": {
        "id": "PhAwn6GtvH-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to set the stopping condition?\n",
        "\n",
        "There is a trade off.  The statistical uncertainties (correlated between the unfolded points) grow with every iteration, but unfolding further does allow more time to reach a stable working point.  Usually,\n",
        "one takes a minimum in:\n",
        "$$\\sum_{bins} \\left(\\sigma^2_{stat} + \\sigma^2_{conv}\\right)$$\n",
        "where $\\sigma_{stat}$ is statistical uncertainty from a bin and $\\sigma_{conv}$ is the difference between the unfolded results for $n$ and $n-1$ iterations.\n",
        "\n",
        "In another way of doing it, Pyunfold implements a variety of test statistics to determine the stopping condition.\n",
        "\n",
        "One wants to iterate as few times as necessary because over-iterating causes large fluctuations/uncertainties.\n",
        "\n"
      ],
      "metadata": {
        "id": "XGldIwkzV1b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we've done so far is set up a nice example but we have been unfolding our MC_reco sample with a response matrix generated from MC_reco and MC_true.  No where has anything like a real measurement come in.  \n",
        "\n",
        "Here we'll go back to the data_true sample we generated above and generate data_reco.  This sample was generated independently from MC_true and so this is the situation that one would have with a\n",
        "real measurement scenario.  We will also generate an independent noise sample (but with the same parameters as above).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ayZVySGZgba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_noise2 = np.random.normal(loc=noise_shift, scale=noise_width, size=num_samples)\n",
        "data_reco = data_true * random_noise2\n",
        "data_reco_hist, _ = np.histogram(data_reco, bins=bins)\n",
        "data_reco_err_hist = np.sqrt(data_reco_hist)"
      ],
      "metadata": {
        "id": "UYStb_JD5vnK"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_unfolded_results = []\n",
        "data_closure = []\n",
        "data_closure_err = []\n",
        "for i in range(len(ts_values)):\n",
        "  tmp_results = iterative_unfold(data=data_reco_hist,\n",
        "                                    data_err=data_reco_err_hist,\n",
        "                                    response=response,\n",
        "                                    response_err=response_err,\n",
        "                                    efficiencies=efficiencies,\n",
        "                                    ts_stopping=ts_values[i],\n",
        "                                    prior=alt_prior_hist,\n",
        "                                    #ts='rmd',\n",
        "                                    efficiencies_err=efficiencies_err,\n",
        "                                    callbacks=[Logger()])\n",
        "  data_unfolded_results.append(tmp_results)\n",
        "  tmp_closure = tmp_results['unfolded'] / data_true_hist\n",
        "  tmp_closure_err = tmp_results['sys_err'] / data_true_hist\n",
        "  data_closure.append(tmp_closure)\n",
        "  data_closure_err.append(tmp_closure_err)"
      ],
      "metadata": {
        "id": "5BAdhN7W6IRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = cm.rainbow(np.linspace(0, 1, len(ts_values)))\n",
        "print(type(colors))\n",
        "fig, ax = plt.subplots(1,2,figsize=(15, 10))\n",
        "ax[0].step(np.arange(num_bins), data_true_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='data True distribution')\n",
        "ax[0].step(np.arange(num_bins), data_reco_hist, where='mid', lw=3,\n",
        "        alpha=0.7, label='data reco distribution')\n",
        "for i in range(len(ts_values)):\n",
        "  labelwords = 'unfolded, stop: ' + str(ts_values[i])\n",
        "  ax[0].errorbar(np.arange(num_bins), data_unfolded_results[i]['unfolded'],\n",
        "            yerr=data_unfolded_results[i]['sys_err'],\n",
        "            alpha=0.7,\n",
        "            elinewidth=3,\n",
        "            capsize=4,\n",
        "                 color=colors[i],\n",
        "            ls='None', marker='.', ms=10,\n",
        "            label=labelwords)\n",
        "\n",
        "  ax[0].set(xlabel='X bins', ylabel='Counts')\n",
        "  ax[1].errorbar(np.arange(num_bins),data_closure[i], yerr=data_closure_err[i], color=colors[i])\n",
        "  ax[1].set(xlabel='X bins', ylabel='unfolded / truth')\n",
        "fig.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5B86dXA56u-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in the red points it's possible to see the effects of iterating too many times (or setting the stopping condition too tight) because the point to point fluctuations have grown very large."
      ],
      "metadata": {
        "id": "Nn2V7m9Ea1Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More than one dimension\n",
        "\n",
        "The case above is for a single dimension.  Perhaps that is measuring the energy distribution of measured particles.  There are a lot of situations where unfolding could be in multiple dimensions:\n",
        "\n",
        "- consider measuring both the position and energy of a particle.  If the energy measurement is correlated with the position measurement than a two-dimensional unfolding might be warrented.\n",
        "- consider the measurement of two correlated particles, the measurement of one could impact the measurement of the other\n",
        "\n",
        "It's straightforward to conceptualize a single dimensional unfolding.  The same procedure works in multiple dimensions, though it becomes more difficult to visualize because the unfolding matrix, $\\vec{R}$ is in $2N$ dimensions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IHzhjLZmwbYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other types of unfolding\n",
        "\n",
        "One drawback of the histogram-based Bayesian approach\n",
        "discussed here is the histograms become very large and often sparsely populated in a multi-dimensional unfolding.  \n",
        "\n",
        "Another drawback of this approach is that unfolding can tell you only about distribution level quantities, nothing is sensitive to the details of a particular event.\n",
        "\n",
        "Approaches using unbinned, machine learning approaches have tried to improve upon both of these concerns.  A well-known approach is [OmniFold](https://inspirehep.net/literature/1766424) ([package](https://pypi.org/project/omnifold/)) which promises: \"We introduce OmniFold, an unfolding method that iteratively reweights a simulated dataset, using machine learning to capitalize on all available information. Our approach is unbinned, works for arbitrarily high-dimensional data, and naturally incorporates information from the full phase space.\".  This approach is inspired by issues in high-energy physics but useable in other fields as well.\n"
      ],
      "metadata": {
        "id": "M3lt5SoybQmR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiildZLrozYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}