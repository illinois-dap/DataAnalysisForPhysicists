{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo and Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from numpy import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange # prints a nice loading bar for the notebook\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Introduction</span>\n",
    "\n",
    "Monte Carlo (MC) integration and other sampling techniques are important tools for computing complex intgerals that arising in many areas of science. In this lecture, we will study Monte Carlo integration methods and re-visit Markov Chain MC with some specific physics examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Monte Carlo Integration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical integration uses the rectangle approximation to find the area under a curve.  The analytical notation we are used to seeing for a definite integral\n",
    "\n",
    "$$ \\Large\n",
    "F = \\int_a^b f(x) dx\n",
    "$$\n",
    "\n",
    "can be expressed as a [numerical approximation that adds up $n$ rectangles under the curve $f(x)$](https://mathworld.wolfram.com/NumericalIntegration.html).  The more rectangles used to calculate the area, the better the approximation becomes.\n",
    "\n",
    "Monte Carlo Integration is a process of solving integrals having numerous values to integrate upon. The Monte Carlo process uses the theory of large numbers and random sampling to approximate values that are very close to the actual solution of the integral. \n",
    "\n",
    "Monte Carlo Integration improves above the integration approach by randomly picking which rectangles to add up next and approximating $F$ as $\\langle F^N \\rangle$:\n",
    "\n",
    "$$ \\Large\n",
    "\\langle F^N \\rangle = ~\\frac{1}{N} \\sum_{i=1}^{N} \\frac{f(X_i)}{p(X_i)} \\\\\n",
    "~~~~~~~~~~~~~~~ = ~\\frac{1}{N} \\sum_{i=1}^{N} \\frac{f(X_i)}{1 / (b-a)} \\\\\n",
    "~~~~~~~~~~~~~~~ = ~\\frac{b-a}{N} \\sum_{i=1}^{N} f(X_i)\n",
    "$$\n",
    "$$ \\Large\n",
    "\\Rightarrow\n",
    "~~~\\boxed{\\langle F^N \\rangle = \\frac{b-a}{N} \\sum_{i=1}^{N} f(X_i)} ~~~~~~~~~~~~~ \\text{(Monte Carlo Estimator)}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of times a new value $X_i$ is chosen from a probability distribution for range $a$ to $b$.  Therefore, \n",
    "\n",
    "$$ \\Large\n",
    "{\\text{lim}}_{N→∞} \\langle F^N \\rangle = F\n",
    "$$\n",
    "\n",
    "The question becomes, what is the best way to choose $X_i$ to match the real system?  The goal is to get the best approximation as quickly as possible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Lets appoximate the definite integral using the Monte Carlo integration method:\n",
    "$$ \\Large\n",
    "\\int \\limits_0^\\pi \\sin x ~dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = np.pi # gets the value of pi \n",
    "N = 1000\n",
    "\n",
    "# array of zeros of length N \n",
    "ar = np.zeros(N) \n",
    "\n",
    "# iterating over each Value of ar and filling \n",
    "# it with a random value between the limits a \n",
    "# and b \n",
    "for i in range (len(ar)): \n",
    "\tar[i] = random.uniform(a,b) \n",
    "\n",
    "# variable to store sum of the functions of \n",
    "# different values of x \n",
    "integral = 0.0\n",
    "\n",
    "# function to calculate the sin of a particular \n",
    "# value of x \n",
    "def f(x): \n",
    "\treturn np.sin(x) \n",
    "\n",
    "# iterates and sums up values of different functions \n",
    "# of x \n",
    "for i in ar: \n",
    "\tintegral += f(i) \n",
    "\n",
    "# we get the answer by the formula derived adobe \n",
    "ans = (b-a)/float(N)*integral \n",
    "\n",
    "# prints the solution \n",
    "print (\"The value calculated by monte carlo integration is {}.\".format(ans)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value obtained is very close to the actual answer of the integral which is 2.0. \n",
    "\n",
    "Now if we want to visualize the integration using a histogram, we can do so by using the matplotlib library. Again we import the modules, define the limits of integration and write the sin function for calculating the sin value for a particular value of x. Next, we take an array that has variables representing every beam of the histogram. Then we iterate through N values and repeat the same process of creating a zeros array, filling it with random x values, creating an integral variable adding up all the function values, and getting the answer N times, each answer representing a beam of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = np.pi # gets the value of pi \n",
    "N = 1000\n",
    "\n",
    "# function to calculate the sin of a particular \n",
    "# value of x \n",
    "def f(x): \n",
    "\treturn np.sin(x) \n",
    "\n",
    "# list to store all the values for plotting \n",
    "plt_vals = [] \n",
    "\n",
    "# we iterate through all the values to generate \n",
    "# multiple results and show whose intensity is \n",
    "# the most. \n",
    "for i in range(N): \n",
    "\t\n",
    "\t#array of zeros of length N \n",
    "\tar = np.zeros(N) \n",
    "\n",
    "\t# iterating over each Value of ar and filling it \n",
    "\t# with a random value between the limits a and b \n",
    "\tfor i in range (len(ar)): \n",
    "\t\tar[i] = random.uniform(a,b) \n",
    "\n",
    "\t# variable to store sum of the functions of different \n",
    "\t# values of x \n",
    "\tintegral = 0.0\n",
    "\n",
    "\t# iterates and sums up values of different functions \n",
    "\t# of x \n",
    "\tfor i in ar: \n",
    "\t\tintegral += f(i) \n",
    "\n",
    "\t# we get the answer by the formula derived adobe \n",
    "\tans = (b-a)/float(N)*integral \n",
    "\n",
    "\t# appends the solution to a list for plotting the graph \n",
    "\tplt_vals.append(ans) \n",
    "\n",
    "# details of the plot to be generated \n",
    "# sets the title of the plot \n",
    "plt.title(\"Distributions of areas calculated\") \n",
    "\n",
    "# 3 parameters (array on which histogram needs \n",
    "plt.hist (plt_vals, bins=30, ec=\"black\") \n",
    "\n",
    "# to be made, bins, separators colour between the \n",
    "# beams) \n",
    "# sets the label of the x-axis of the plot \n",
    "plt.xlabel(\"Areas\") \n",
    "plt.show() # shows the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Lets appoximate the definite integral\n",
    "$$ \\Large\n",
    "\\int \\limits_0^1 x^2 ~dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = 1\n",
    "N = 1000\n",
    "\n",
    "# array of zeros of length N \n",
    "ar = np.zeros(N) \n",
    "\n",
    "# iterating over each Value of ar and filling \n",
    "# it with a random value between the limits a \n",
    "# and b \n",
    "for i in range(len(ar)): \n",
    "\tar[i] = random.uniform(a, b) \n",
    "\n",
    "# variable to store sum of the functions of \n",
    "# different values of x \n",
    "integral = 0.0\n",
    "\n",
    "# function to calculate the sin of a particular \n",
    "# value of x \n",
    "def f(x): \n",
    "\treturn x**2\n",
    "\n",
    "# iterates and sums up values of different \n",
    "# functions of x \n",
    "for i in ar: \n",
    "\tintegral += f(i) \n",
    "\n",
    "# we get the answer by the formula derived adobe \n",
    "ans = (b-a)/float(N)*integral \n",
    "\n",
    "# prints the solution \n",
    "print(\"The value calculated by monte carlo integration is {}.\".format(ans)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = 1\n",
    "N = 1000\n",
    "\n",
    "# function to calculate x^2 of a particular value \n",
    "# of x \n",
    "def f(x): \n",
    "\treturn x**2\n",
    "\n",
    "# list to store all the values for plotting \n",
    "plt_vals = [] \n",
    "\n",
    "# we iterate through all the values to generate \n",
    "# multiple results and show whose intensity is \n",
    "# the most. \n",
    "for i in range(N): \n",
    "\t\n",
    "\t# array of zeros of length N \n",
    "\tar = np.zeros(N) \n",
    "\n",
    "\t# iterating over each Value of ar and filling \n",
    "\t# it with a random value between the limits a \n",
    "\t# and b \n",
    "\tfor i in range (len(ar)): \n",
    "\t\tar[i] = random.uniform(a,b) \n",
    "\n",
    "\t# variable to store sum of the functions of \n",
    "\t# different values of x \n",
    "\tintegral = 0.0\n",
    "\n",
    "\t# iterates and sums up values of different functions \n",
    "\t# of x \n",
    "\tfor i in ar: \n",
    "\t\tintegral += f(i) \n",
    "\n",
    "\t# we get the answer by the formula derived adobe \n",
    "\tans = (b-a)/float(N)*integral \n",
    "\n",
    "\t# appends the solution to a list for plotting the \n",
    "\t# graph \n",
    "\tplt_vals.append(ans) \n",
    "\n",
    "# details of the plot to be generated \n",
    "# sets the title of the plot \n",
    "plt.title(\"Distributions of areas calculated\") \n",
    "\n",
    "# 3 parameters (array on which histogram needs \n",
    "# to be made, bins, separators colour between \n",
    "# the beams) \n",
    "plt.hist (plt_vals, bins=30, ec=\"black\") \n",
    "\n",
    "# sets the label of the x-axis of the plot \n",
    "plt.xlabel(\"Areas\") \n",
    "plt.show() # shows the plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Multidimensional Monte Carlo integration and variance scaling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the Monte Carlo variance of the approximation as\n",
    "$$ \\Large\n",
    "v_N = \\frac{1}{N^2} \\sum_{i=1}^N \\Biggl[ (f(x_i) - \\bar{f_N})^2 \\Biggr]\n",
    "$$\n",
    "\n",
    "Also, from the Central Limit Theorem,\n",
    "$$ \\Large\n",
    "\\frac{\\bar{f_N} - E[f(X)]}{\\sqrt{v_N}} \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "The convergence of Monte Carlo integration is $O(\\sqrt{N})$ and independent of the dimensionality. Hence Monte Carlo integration generally beats numerical integration for moderate- and high-dimensional integration since numerical integration (quadrature) converges as $O(N^d)$. Even for low dimensional problems, Monte Carlo integration may have an advantage when the volume to be integrated is concentrated in a very small region and we can use information from the distribution to draw samples more often in the region of importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: 3-D Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large\n",
    "\\int\\limits_{x_0}^{x_1} ~ \\int\\limits_{y_0}^{y_1} ~ \\int\\limits_{z_0}^{z_1} f(x, y, z) dx~dy~dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform 3-D random variable:\n",
    "$$\\Large\n",
    "X_i \\sim p(x,y,z) = \\frac{1}{x_1 - x_0} ~ \\frac{1}{y_1 - y_0} ~ \\frac{1}{z_1 - z_0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic 3-D estimator:\n",
    "$$ \\Large\n",
    "F_N = \\frac{(x_1 - x_0)(y_1 - y_0)(z_1 - z_0)}{N} ~ \\sum\\limits_{i=1}^{N} f(X_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalizes to abitrary N-dimensional PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Variance and Bias in Monte Carlo integration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in knowing how many iterations it takes for Monte Carlo integration to “converge” and the accuracy of the calculation. To do this, we would like some estimate of the variance and to ensure it is unbiased. It is useful to inspect such plots. One simple way to get confidence intervals for the plot of Monte Carlo estimate against number of iterations is simply to do many such simulations.\n",
    "\n",
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Using Monte Carlo methods, estimate the integral of the function\n",
    "$$ \\Large\n",
    "f(x) = x \\cos 7x + \\sin 13x, \\ \\ \\ 0 \\le x \\le 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.cos(71*x) + np.sin(13*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, f(x))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Single Monte Carlo estimate</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = f(np.random.random(n))\n",
    "y = 1.0/n * np.sum(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Using multiple independent sequences to monitor convergence</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vary the sample size from 1 to 100 and calculate the value of $y=\\sum x / n$ for 1000 replicates. We then plot the 2.5th and 97.5th percentile of the 1000 values of $y$ to see how the variation in $y$ changes with sample size. The blue lines indicate the 2.5th and 97.5th percentiles, and the red line a sample path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "reps = 1000\n",
    "\n",
    "x = f(np.random.random((n, reps)))\n",
    "y = 1/np.arange(1, n+1)[:, None] * np.cumsum(x, axis=0)\n",
    "upper, lower = np.percentile(y, [2.5, 97.5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, n+1), y, c='grey', alpha=0.02)\n",
    "plt.plot(np.arange(1, n+1), y[:, 0], c='red', linewidth=1);\n",
    "plt.plot(np.arange(1, n+1), upper, 'b', np.arange(1, n+1), lower, 'b')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Proof that Monte Carlo Estimator is Unbiased</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to prove that the expectation value of the Monte Carlo estimator is the desired integral (i.e. it is unbiased). \n",
    "\n",
    "First recall some properties of the expectation value $E$:\n",
    "$$ \\Large\n",
    "E ~ \\Biggl[ \\sum\\limits_i Y_i \\Biggr] = \\sum\\limits_i E\\bigl[ Y_i \\bigr]   ~~~~~~~~~~~~~~~~~~~~~ E~\\Bigl[ aY \\Bigr] = a E~\\Bigl[ Y \\Bigr]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "$$ \\Large E~\\Bigl[ F_N \\Bigr] = E ~ \\Biggl[ \\frac{1}{N} ~ \\sum\\limits_{i=1}^N \\frac{f(X_i)}{p(X_I)} \\Biggr] \\\\\n",
    "~~~~~~~~~~~~~~ = \\frac{1}{N} ~ \\sum\\limits_{i=1}^N ~E~ \\Biggl[ \\frac{f(X_i)}{p(X_I)} \\Biggr] \\\\\n",
    "~~~~~~~~~~~~~~~~~~~~~~~ = \\frac{1}{N} ~ \\sum\\limits_{i=1}^N ~\\int\\limits_a^b \\frac{f(x)}{p(x)} ~p(x) ~dx \\\\\n",
    "~~~~~~~~~~~~~~ = \\frac{1}{N} ~ \\sum\\limits_{i=1}^N ~\\int\\limits_a^b f(x)  ~dx \\\\\n",
    "~~ = \\int\\limits_a^b f(x)  ~dx \\\\\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Change of Variables</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cauchy distribution is given by\n",
    "$$ \\Large\n",
    "f(x) = \\frac{1}{\\pi (1 + x^2)}, \\ \\ -\\infty \\lt x \\lt \\infty\n",
    "$$\n",
    "\n",
    "Suppose we want to integrate the tail probability $P(X>3)$ using Monte Carlo. One way to do this is to draw many samples form a Cauchy distribution, and count how many of them are greater than 3, but this is extremely inefficient.\n",
    "\n",
    "Only 10% of samples will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true = 1 - stats.cauchy().cdf(3)\n",
    "print(h_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "x = stats.cauchy().rvs(n)\n",
    "h_mc = 1.0/n * np.sum(x > 3)\n",
    "h_mc, np.abs(h_mc - h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">A change of variables lets us use 100% of draws</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to estimate the quantity\n",
    "$$ \\Large\n",
    "\\int_3^\\infty \\frac{1}{\\pi (1 + x^2)} dx\n",
    "$$\n",
    "\n",
    "Using the substitution $y=3/x$ (and a little algebra), we get\n",
    "$$ \\Large\n",
    "\\int_0^1 \\frac{3}{\\pi(9 + y^2)} dy\n",
    "$$\n",
    "\n",
    "Hence, a much more efficient MC estimator is\n",
    "$$ \\Large\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\frac{3}{\\pi(9 + y_i^2)}\n",
    "$$\n",
    "\n",
    "where $y_i \\sim \\square(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stats.uniform().rvs(n)\n",
    "h_cv = 1.0/n * np.sum(3.0/(np.pi * (9 + y**2)))\n",
    "h_cv, np.abs(h_cv - h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Monte Carlo Swindles</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from change of variables, there are several general techniques for variance reduction, sometimes known as Monte Carlo \"swindles\" since these methods improve the accuracy and convergence rate of Monte Carlo integration without increasing the number of Monte Carlo samples. Some Monte Carlo swindles are:\n",
    "\n",
    "- importance sampling\n",
    "\n",
    "- stratified sampling\n",
    "\n",
    "- control variates\n",
    "\n",
    "- antithetic variates\n",
    "\n",
    "- conditioning swindles including Rao-Blackwellization and independent variance decomposition\n",
    "\n",
    "Most of these techniques are not particularly computational in nature, so we will not cover them in the course. I expect you will learn them elsewhere. We will illustrate importance sampling and antithetic variables here as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Antithetic variables</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind antithetic variables is to choose two sets of random numbers that are negatively correlated, then take their average, so that the total variance of the estimator is smaller than it would be with two sets of independent and identically distributed (IID) random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.cos(71*x) + np.sin(13*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import sin, cos, symbols, integrate\n",
    "\n",
    "x = symbols('x')\n",
    "sol = integrate(x * cos(71*x) + sin(13*x), (x, 0,1)).evalf(16)\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just vanilla Monte Carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "u = np.random.random(n)\n",
    "x = f(u)\n",
    "y = 1.0/n * np.sum(x)\n",
    "y, abs(y-sol)/sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using antithetic variables for the first half of $u$ supplemented with $1-u$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.r_[u[:n//2], 1-u[:n//2]]\n",
    "x = f(u)\n",
    "y = 1.0/n * np.sum(x)\n",
    "y, abs(y-sol)/sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because the random draws are now negatively correlated, and hence the sum of the variances will be less than in the IID case, while the expectation is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Importance Sampling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Monte Carlo sampling evaluates\n",
    "\n",
    "$$ \\Large\n",
    "E[g(X)] = \\int_X g(x)\\, p(x) \\, dx\n",
    "$$\n",
    "\n",
    "Using another distribution $h(x)$ which is the so-called “importance function”, we can rewrite the above expression as an expectation with respect to $h$\n",
    "\n",
    "$$ \\Large\n",
    "E_p[f(x)] \\ = \\ \\int_X f(x) \\frac{p(x)}{h(x)} h(x) dx \\ = \\ E_h\\left[ \\frac{f(X) p(X)}{h(X)} \\right] \n",
    "$$\n",
    "\n",
    "giving us the new estimator\n",
    "\n",
    "$$ \\Large\n",
    "\\bar{f_n} = \\frac{1}{N} \\sum_{i=1}^n \\frac{p(x_i)}{h(x_i)} f(x_i)\n",
    "$$\n",
    "\n",
    "where $x_i \\sim f$ is a draw from the density $h$.\n",
    "\n",
    "This is helpful if the distribution $h$ has a similar shape as the function $f(x)$ that we are integrating over, since we will draw more samples from places where the integrand makes a larger or more “important” contribution. This is very dependent on a good choice for the importance function $h$.\n",
    "\n",
    "Two simple choices for $h$ are scaling\n",
    "\n",
    "$$ \\Large\n",
    "h(x) = \\frac{1}{a} ~p(x/a)\n",
    "$$\n",
    "\n",
    "and translation\n",
    "\n",
    "$$ \\Large\n",
    "h(x) = p ~(x - a)\n",
    "$$\n",
    "\n",
    "In these cases, the parameter a is typically chosen using some adaptive algorithm, giving rise to adaptive importance sampling. Alternatively, a different distribution can be chosen as shown in the example below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Suppose we want to estimate the tail probability of $\\square (0,1)$ for $P(X>5)$. \n",
    "\n",
    "Regular MC integration using samples from $\\square (0,1)$ is hopeless since nearly all samples will be rejected. However, we can use the exponential density truncated at 5 as the importance function and use importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(4, 10, 100)\n",
    "plt.plot(x, stats.expon(5).pdf(x))\n",
    "plt.plot(x, stats.norm().pdf(x))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Expected answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect about 3 draws out of 10,000,000 from $\\square (0,1)$ to have a value greater than 5. Hence simply sampling from $\\square (0,1)$ is hopelessly inefficient for Monte Carlo integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true = 1 - stats.norm().cdf(5)\n",
    "h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Using direct Monte Carlo integration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "y = stats.norm().rvs(n)\n",
    "h_mc = 1.0/n * np.sum(y > 5)\n",
    "# estimate and relative error\n",
    "h_mc, np.abs(h_mc - h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Using importance sampling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "y = stats.expon(loc=5).rvs(n)\n",
    "h_is = 1.0/n * np.sum(stats.norm().pdf(y)/stats.expon(loc=5).pdf(y))\n",
    "# estimate and relative error\n",
    "h_is, np.abs(h_is- h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Quasi-random numbers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the convergence of Monte Carlo integration is $O(\\sqrt{N})$. One issue with simple Monte Carlo is that randomly chosen points tend to be clumped. Clumping reduces accuracy since nearby points provide little additional information about the function begin estimated. One way to address this is to split the space into multiple integration regions, then sum them up. This is known as <span style=\"color:Violet\">stratified sampling</span>. Another alternative is to use <span style=\"color:Violet\">quasi-random numbers</span> which fill space more efficiently than random sequences.\n",
    "\n",
    "It turns out that if we use quasi-random or low discrepancy sequences, we can get convergence approaching $O(1/N)$. There are several such generators, but their use in statistical settings is limited to cases where we are integrating with respect to uniform distributions. The regularity can also give rise to errors when estimating integrals of periodic functions. However, these quasi-Monte Carlo methods are used in computational finance models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ghalton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ghalton\n",
    "\n",
    "gen = ghalton.Halton(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "xs = np.random.random((100,2))\n",
    "plt.scatter(xs[:, 0], xs[:,1])\n",
    "plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "plt.title('Pseudo-random', fontsize=20)\n",
    "plt.subplot(122)\n",
    "ys = np.array(gen.get(100))\n",
    "plt.scatter(ys[:, 0], ys[:,1])\n",
    "plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "plt.title('Quasi-random', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true = 1 - stats.cauchy().cdf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = stats.uniform().rvs((n, 5))\n",
    "y = 3.0/(np.pi * (9 + x**2))\n",
    "h_mc = np.sum(y, 0)/n\n",
    "list(zip(h_mc, 100*np.abs(h_mc - h_true)/h_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen1 = ghalton.Halton(1)\n",
    "x = np.reshape(gen1.get(n*5), (n, 5))\n",
    "y = 3.0/(np.pi * (9 + x**2))\n",
    "h_qmc = np.sum(y, 0)/n\n",
    "list(zip(h_qmc, 100*np.abs(h_qmc - h_true)/h_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Vegas Method</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VEGAS algorithm, due to G. Peter Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral. \n",
    "\n",
    "The VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function $|f|$ so that the points are concentrated in the regions that make the largest contribution to the integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: 4-D Monte Carlo integration with Vegas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we illustrate the use of [vegas](https://vegas.readthedocs.io/en/latest/vegas.html#module-vegas) by estimating the following integral:\n",
    "$$ \\Large\n",
    "C \\int\\limits_{-1}^{1} ~dx_0 ~ \\int\\limits_{0}^{1} ~dx_1 ~ \\int\\limits_{0}^{1} ~dx_2 ~ \\int\\limits_{0}^{1} ~dx_3 ~ e^{-100 \\sum_d (x_d - 0.5)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the integrand $f(x)$ where $x[d]$ specifies a point in the 4-dimensional space. \n",
    "\n",
    "We then create an integrator `integ` which is an integration operator that can be applied to any 4-dimensional function. It is where we specify the integration volume. \n",
    "\n",
    "Finally we apply `integ` to our integrand $f(x)$, telling the integrator to estimate the integral using `nitn=10` iterations of the vegas algorithm, each of which uses no more than `neval=1000` evaluations of the integrand. Each iteration produces an independent estimate of the integral. \n",
    "\n",
    "The final estimate is the weighted average of the results from all 10 iterations, and is returned by `integ(f ...)`. The call `result.summary()` returns a summary of results from each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vegas\n",
    "\n",
    "import vegas\n",
    "\n",
    "def f(x):\n",
    "    dx2 = 0\n",
    "    for d in range(4):\n",
    "        dx2 += (x[d] - 0.5) ** 2\n",
    "    return np.exp(-dx2 * 100.) * 1013.2118364296088\n",
    "\n",
    "# seed the random number generator so results reproducible\n",
    "np.random.seed((1, 2, 3))\n",
    "\n",
    "# assign integration volume to integrator\n",
    "integ = vegas.Integrator([[-1., 1.], [0., 1.], [0., 1.], [0., 1.]])\n",
    "\n",
    "# adapt to the integrand; discard results\n",
    "integ(f, nitn=5, neval=1000)\n",
    "\n",
    "# do the final integral\n",
    "result = integ(f, nitn=10, neval=1000)\n",
    "print(result.summary())\n",
    "print('result = %s    Q = %.2f' % (result, result.Q))\n",
    "integ.map.show_grid(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgements\n",
    "* Initial version: Mark Neubauer\n",
    "* From APS DSECOP materials and https://people.duke.edu/~ccc14/sta-663-2016/15C_MonteCarloIntegration.html\n",
    "\n",
    "© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
